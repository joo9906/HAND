from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.api import route
from model_loader import model, tokenizer
from contextlib import asynccontextmanager
import torch
import os

@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        print("ğŸš€ ì„œë²„ ì‹œì‘ ì¤‘â€¦ ëª¨ë¸ Warm-up ì¤‘ì…ë‹ˆë‹¤.")
        
        # í† í¬ë‚˜ì´ì € ì…ë ¥ ì¤€ë¹„
        inputs = tokenizer("ì˜¤ëŠ˜ í•´ê°€ ë‚˜ì™€ì„œ ê¸°ë¶„ ì¢‹ì•„.", return_tensors="pt")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • ë° ì´ë™
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # gradient ë¹„í™œì„±í™” í›„ ëª¨ë¸ ì‹¤í–‰
        model.eval()
        with torch.no_grad():
            _ = model(**inputs)
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ë° Warm-up ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ Warm-up ì‹¤íŒ¨: {e}")

    # FastAPI ì•±ì´ ì‹¤í–‰ë˜ëŠ” ë™ì•ˆ ìœ ì§€
    yield

    # ì„œë²„ ì¢…ë£Œ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    try:
        torch.cuda.empty_cache()
        print("ğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘â€¦ GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ.")
    except Exception as e:
        print(f"âš ï¸ ì¢…ë£Œ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")


app = FastAPI(lifespan=lifespan, title="AI Server")

# CORS ì„¤ì •
allowed_origins = os.getenv("CORS_ORIGINS", "https://gatewaytohand.store").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://gatewaytohand.store/api/v1/",
        "http://localhost:8000",
        "http://127.0.0.1:8000"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ë¼ìš°í„° ë“±ë¡
app.include_router(route.router, prefix="/ai")